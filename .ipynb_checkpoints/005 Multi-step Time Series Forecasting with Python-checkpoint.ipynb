{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Setting up packages for data manipulation and machine learning\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.layers import LSTM, Dense, TimeDistributed, Dropout, Activation\n",
    "\n",
    "# Creating the sample sinus curve dataset\n",
    "steps = 300\n",
    "gradient = 0.02\n",
    "list_a = []\n",
    "for i in range(0, steps, 1):\n",
    "    y = round(gradient * i + math.sin(math.pi * 0.125 * i), 5)\n",
    "    list_a.append(y)\n",
    "df = pd.DataFrame({\"valid\": list_a}, columns=[\"valid\"])\n",
    "\n",
    "# Visualizing the data\n",
    "fig, ax1 = plt.subplots(figsize=(16, 4))\n",
    "ax1.xaxis.set_major_locator(plt.MaxNLocator(30))\n",
    "plt.title(\"Sinus Data\")\n",
    "plt.plot(df[[\"valid\"]], color=\"#039dfc\", linewidth=3.0)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_tain.shape: (130, 110, 1) -- y_tain.shape: (130,)\n"
     ]
    }
   ],
   "source": [
    "# Settings\n",
    "epochs = 12; batch_size = 1; lstm_neuron_number = 110\n",
    "\n",
    "# Get the number of rows to train the model on 80% of the data\n",
    "npdataset = df.values\n",
    "training_data_length = math.ceil(len(npdataset) * 0.8)\n",
    "\n",
    "# Transform features by scaling each feature to a range between 0 and 1\n",
    "mmscaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = mmscaler.fit_transform(npdataset)\n",
    "\n",
    "# Create a scaled training data set\n",
    "train_data = scaled_data[0:training_data_length, :]\n",
    "\n",
    "# Split the data into x_train and y_train data sets\n",
    "x_train = []\n",
    "y_train = []\n",
    "trainingdatasize = len(train_data)\n",
    "for i in range(lstm_neuron_number, trainingdatasize):\n",
    "    x_train.append(\n",
    "        train_data[i - lstm_neuron_number : i, 0]\n",
    "    )  # contains lstm_neuron_number values 0-lstm_neuron_number\n",
    "    y_train.append(train_data[i, 0])  # contains all other values\n",
    "\n",
    "# Convert the x_train and y_train to numpy arrays\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# Reshape the data\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "print(\"x_tain.shape: \" + str(x_train.shape) + \" -- y_tain.shape: \" + str(y_train.shape))\n",
    "\n",
    "# Configure and compile the neural network model\n",
    "model1 = Sequential()\n",
    "model1.add(\n",
    "    LSTM(lstm_neuron_number, return_sequences=False, input_shape=(x_train.shape[1], 1))\n",
    ")\n",
    "model1.add(Dense(1))\n",
    "model1.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "\n",
    "# Create the test data set\n",
    "test_data = scaled_data[training_data_length - lstm_neuron_number :, :]\n",
    "\n",
    "# Create the data sets x_test and y_test\n",
    "x_test = []\n",
    "y_test = npdataset[training_data_length:, :]\n",
    "for i in range(lstm_neuron_number, len(test_data)):\n",
    "    x_test.append(test_data[i - lstm_neuron_number : i, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model1.fit(x_train, y_train, batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data, so that we get an array with multiple test datasets\n",
    "x_test = np.array(x_test)\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "# Get the predicted values\n",
    "predictions = model1.predict(x_test)\n",
    "predictions = mmscaler.inverse_transform(predictions)\n",
    "\n",
    "# Get the root mean squarred error (RMSE) and the meadian error (ME)\n",
    "rmse = np.sqrt(np.mean(predictions - y_test) ** 2)\n",
    "me = np.median(y_test - predictions)\n",
    "print(\"me: \" + str(round(me, 4)) + \", rmse: \" + str(round(rmse, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "train = df[:training_data_length]\n",
    "valid = df[training_data_length:]\n",
    "valid.insert(1, \"Predictions\", predictions, True)\n",
    "fig, ax1 = plt.subplots(figsize=(32, 5), sharex=True)\n",
    "yt = train[[\"valid\"]]\n",
    "yv = valid[[\"valid\", \"Predictions\"]]\n",
    "ax1.tick_params(axis=\"x\", rotation=0, labelsize=10, length=0)\n",
    "plt.title(\"Predictions vs Ground Truth\", fontsize=18)\n",
    "plt.plot(yv[\"Predictions\"], color=\"#F9A048\")\n",
    "plt.plot(yv[\"valid\"], color=\"#A951DC\")\n",
    "plt.legend([\"Ground Truth\", \"Train\"], loc=\"upper left\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation loss values\n",
    "fig, ax = plt.subplots(figsize=(5, 5), sharex=True)\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.title(\"Model loss\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "ax.xaxis.set_major_locator(plt.MaxNLocator(epochs))\n",
    "plt.legend([\"Train\", \"Test\"], loc=\"upper left\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings and Model Labels\n",
    "rolling_forecast_range = 30\n",
    "titletext = \"Forecast Chart Model A\"\n",
    "ms = [\n",
    "    [\"epochs\", epochs],\n",
    "    [\"batch_size\", batch_size],\n",
    "    [\"lstm_neuron_number\", lstm_neuron_number],\n",
    "    [\"rolling_forecast_range\", rolling_forecast_range],\n",
    "    [\"layers\", \"LSTM, DENSE(1)\"],\n",
    "]\n",
    "settings_text = \"\"\n",
    "lms = len(ms)\n",
    "for i in range(0, lms):\n",
    "    settings_text += ms[i][0] + \": \" + str(ms[i][1])\n",
    "    if i < lms - 1:\n",
    "        settings_text = settings_text + \",  \"\n",
    "\n",
    "# Making a Multi-Step Prediction\n",
    "new_df = df.filter([\"valid\"])\n",
    "for i in range(0, rolling_forecast_range):\n",
    "    last_values = new_df[-lstm_neuron_number:].values\n",
    "    last_values_scaled = mmscaler.transform(last_values)\n",
    "    X_input = []\n",
    "    X_input.append(last_values_scaled)\n",
    "    X_input = np.array(X_input)\n",
    "    X_test = np.reshape(X_input, (X_input.shape[0], X_input.shape[1], 1))\n",
    "    pred_value = model1.predict(X_input)\n",
    "    pred_value_unscaled = mmscaler.inverse_transform(pred_value)\n",
    "    pred_value_f = round(pred_value_unscaled[0, 0], 4)\n",
    "    next_index = new_df.iloc[[-1]].index.values + 1\n",
    "    new_df = new_df.append(pd.DataFrame({\"valid\": pred_value_f}, index=next_index))\n",
    "    new_df_length = new_df.size\n",
    "forecast = new_df[new_df_length - rolling_forecast_range : new_df_length].rename(\n",
    "    columns={\"valid\": \"Forecast\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the results\n",
    "pred.index = pred.index\n",
    "validxs = valid.copy()\n",
    "dflen = new_df.size - 1\n",
    "validxs.insert(2, \"Forecast\", forecast, True)\n",
    "dfs = pd.concat([validxs, forecast], sort=False)\n",
    "dfs.at[dflen, \"Forecast\"] = dfs.at[dflen, \"Predictions\"]\n",
    "\n",
    "# Zoom in to a closer timeframe\n",
    "dfs = dfs[dfs.index > 200]\n",
    "yt = dfs[[\"valid\"]]\n",
    "yv = dfs[[\"Predictions\"]]\n",
    "yz = dfs[[\"Forecast\"]]\n",
    "xz = dfs[[\"Forecast\"]].index\n",
    "\n",
    "# Visualize the data\n",
    "fig, ax1 = plt.subplots(figsize=(16, 5), sharex=True)\n",
    "ax1.tick_params(axis=\"x\", rotation=0, labelsize=10, length=0)\n",
    "ax1.xaxis.set_major_locator(plt.MaxNLocator(30))\n",
    "plt.title('Forecast Basic Model', fontsize=18)\n",
    "plt.plot(yt, color=\"#039dfc\", linewidth=1.5)\n",
    "plt.plot(yv, color=\"#F9A048\", linewidth=1.5)\n",
    "plt.scatter(xz, yz, color=\"#F332E6\", linewidth=1.0)\n",
    "plt.plot(yz, color=\"#F332E6\", linewidth=0.5)\n",
    "plt.legend([\"Ground Truth\", \"TestPredictions\", \"Forecast\"], loc=\"upper left\")\n",
    "ax1.annotate('ModelSettings: ' + settings_text, xy=(0.06, .015),  xycoords='figure fraction', horizontalalignment='left', verticalalignment='bottom', fontsize=10)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
